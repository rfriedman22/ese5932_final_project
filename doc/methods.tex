\documentclass[12pt]{article}
%\usepackage[latin1]{inputenc}
\usepackage{pdfsync}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{eucal}
\usepackage{indentfirst}
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{sidecap}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{booktabs}
\usepackage{lastpage}
\usepackage[titletoc]{appendix}
\usepackage{soul}
\usepackage{pythontex}

%\usepackage{ulem}
\usepackage[body={9.0in, 9.0in},left=1 in,right=1in]{geometry} % Geometry package for easy page margin setup\setcounter{MaxMatrixCols}{30}
\linespread{1} % interline
\DeclareGraphicsExtensions{.jpg,.pdf,.mps,.png,.eps}
 

%%%%%%%%%%%%%Colors%%%%%%%%%%%%%%%
\usepackage[dvispnames]{xcolor}
\definecolor{pink}{cmyk}{0, 0.7808, 0.4429, 0.1412}
%%%%%%%%%%%%% Page setup %%%%%%%%%%
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
%\lhead{\footnotesize \parbox{11cm}{Draft 1} }
\lfoot{\footnotesize Siqi Zhao\\ szhao@wustl.edu}
%\cfoot{}
%\chead{}
%\rhead{\footnotesize 3}
\rfoot{\footnotesize Page \thepage\ of \pageref{LastPage}}
\newcommand{\noin}{\noindent} 


%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{pythonhighlight}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\usepackage{physics}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


\section{Methods}

\noin As we know from the physics of computed tomography, the measured sinogram image is convolved with several sources of noise, notably electronic noise and structural noise\cite{Duan2013-mi}. Hence, using an iterative method that handles noise is necessary. In formulating the CT reconstruction as an optimization problem, one of the important considerations is to find a regularization method for the ill-posed problem that preserves the edges in the true image, since identifying boundaries of objects (such as the boundary between normal and abnormal tissues) has high diagnostic value.

\vspace{0.2in}

\noin We decided to follow the fast gradient-based algorithm proposed by Beck and Teboulle\cite{Beck2009-id}, and the following section will discuss the specific implementation of their method for CT image reconstruction.

\vspace{0.2in}

\noin Specifically, we chose isotropic TV regularization\footnote{In our presentation, we claimed that we were using the anisotropic TV because we falsely assumed that we were optimizing an 1D vector rather than a 2D image. For the 1D case, isotropic and anisotropic TV are essentially the same. The error was resulted from the fact that the majority of the group members are not in the field of imaging science as are unfamiliar with how matlab implicitly handles an image. And the error of assuming a 1D vector resulted in thinking we can use anisotropic TV as well as creating vertical artifacts in reconstructed images, we have corrected the errors in the implementation and rewritten this section. This has been a good learning opportunity for us both in terms of learning new knowledge as well as learning the process of gaining such knowledge from outside of the field. We also attempted to rewrite the method section to increase clarity, we would appreciate any feedback on this version of the description of the method.}. The original publication of FISTA with TV\cite{Beck2009-id} describes two types of TV regularization, anisotropic and isotropic TV, and the specific python package of choice, pyunloc\cite{pyunlocbox}, implements isotropic TV functionals. We decided to use the implemented TV function in the package and conducted some research on the differences of those two types of TV regularization functions. As being described in \cite{Condat2017-uj}, anisotropic TV is known to favor horizontal and vertical structures for 2D images, and isotropic TV is shown to be a superior choice for regularization for a 2D image that preserves the off-axis edges. 

\vspace{0.2in}

\noin The optimization problem can be formulated as:
\begin{align}
 \underset{x}{min}\{ ||A(x) - d||^2 + 2 \lambda\text{TV}(x)\} \label{eq1}
\end{align}

\noin where
$x \in \mathbb{R}^{m \times n}$ is the true image with the size $\sqrt{m \times n}$.
$d \in \mathbb{R}^{p \times q}$ is the sinogram image where $ p:|rays|\times q:|views|$.
$A \in \mathbb{R}^{m \times n}$ is the forward operator that transforms an image to a sinogram image.
$\lambda$ is the regularization parameter. 
$\text{TV}(x): \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$  is an istropic total variation functional, where 
\begin{align}
    x \in \mathbb{R}^{m\times n}, \text{TV}_{\text{I}}(x) &= \sum_{i=1}^{m-1}\sum_{i=1}^{n-1}\sqrt{(x_{i,j} - x_{i+1,j})^2 + (x_{i,j} - x_{i,j+1})^2} \\
    &+ \sum_{i=1}^{m-1}{|x_{i,n} - x_{i+1, n}|} + \sum_{i=1}^{n-1}{|x_{m,j} - x_{m, j+1}|}
\end{align}

\noin To solve the problem in eq.\ref{eq1}, it requires a method that sovles non-differentiable optimization problems. We decided to use fast iterative shrinkage/thresholding algorithm(FISTA) as the algorithm to solve the optimization problem formulated for this project\cite{Beck2009-id}. FISTA has two main advantages: first, it has a global convergence rate of $\mathcal{O}(\frac{1}{k^2})$ (The proof of convergence rate is detailed in \cite{Beck2009-id}); second, FISTA is suitable for dealing with a non-smooth regularization function. 

\vspace{0.2in}

\noin The general principle of the algorithm will be discussed in this paragraph and the pseudocode for the specific implementation will follow. 

\vspace{0.2in}

\noin We discuss now how FISTA solved the optimization problem in eq.\ref{eq1}. Let $f(x)$ denote the data fidelity term, it is clear that $f(x)$ is smooth and differentiable. The simple iterative method for solving the minimization of $f(x)$ is the gradient descent method:
\begin{align}
    x_k = x_{k-1} - t_k \nabla f(x_{k-1})
\end{align}

\noin As stated in \cite{Beck2009-id}, the optimization problem in eq.\ref{eq1} can be considered as a proximal regulation problem of $f(x_k)$ at $x_k$. Skipping the derivation, we have:
\begin{align}
    x_k = \underset{x}{\mathrm{argmin}}\{\frac{1}{2t_k}||x - (x_{k-1} - t_k \nabla f(x_{k-1})||^2 + \lambda||x||_{tv}\}
\end{align}

\noin Specifically, for FISTA, each update is:

\begin{align}
    x_k &= p_{\text{L}}(y_k) \\
    t_{k+1} &= \frac{1+ \sqrt{1 + 4 t_k^2}}{2} \\
    y_{k+1} &= x_k + \frac{t_k -1}{t_{k+1}}(x_k - x_{k-1})
\end{align}

\noin where $p_{\text{L}} = \underset{x \in \mathbb{E}}{argmin}\{\frac{L}{2} ||x - (y - \frac{1}{L} \nabla f(y)||^2 + ||x||_{tv}\}$

\vspace{0.2in}
\noin Compare to ISTA, FISTA uses the information from past 2 steps instead of 1 step to decide the new start $x_k$, This smarter choice of $x_k$ allows the proximal gradient approaches the minimum faster. 

\vspace{0.2in}

\noin After establishing FISTA is a fast algorithm for solving the optimization problem in eq.\label{eq1}, we discuss the dual approach to solve for isotropic total variation. The optimal solution for the inner problem is:

\begin{align}
    x_k = P_{\text{C}}(d - \lambda \mathcal{L}(p_k,q_k))
\end{align}

where linear operator $\mathcal{L} : \mathbb{R}^{((m-1) \times n)}\mathbb{R}^{(m\times (n-1))} \rightarrow \mathbb{R}^{m \times n}$ is defined by 
\begin{align}
\mathcal{L}(\mathbf{p},\mathbf{q})_{i,j} = p_{i,j} + q_{i,j} - p_{i-1,j} - &q_{i,j-1}\\
& i = 1, \cdots, m, j = 1, \cdots, n
\end{align}

\noin and $P_{\text{C}}$ is an orthogonal projection operator onto a feasible set to ensure smoothness. To apply the dual approach, we need to use the mapping from $(p,q) \rightarrow (r,s)$ to ensure the smoothness. 

\vspace{0.2in}
\noin We found an existing package in python called pyunlocbox \cite{pyunlocbox} to implement the algorithm. This package has all the required components for constructing the functions for data fidelity and regularization terms, the pseudocode is in algorithm\ref{euclid}.

\begin{algorithm}
\caption{TV-FISTA}\label{euclid}
    \begin{algorithmic}
    \State \textbf{Input} $d$, $A$, $\lambda$, $maxiter$, $tol$
    \State $(r_0,s_0) = (p_0,q_0) = (0_{(m-1) \times n},0_{m\times (n-1)})$, $t_1 = 1$
    \If {$k \leq maxiter$}
    \BState \emph{Forward}
    \State \textbf{Compute} $x_f = \frac{2}{\lambda}A^{T}(A(x_{k-1})-d)$
    \BState \emph{Backward}
    \State $x_k = x_f - \frac{1}{\lambda} \div (r_{k},s_{k})$
    \State $obj_k = \frac{1}{2}||x_f -x_k ||^2 + \lambda ||\nabla x_k||^2$
    \State $tol_k = \frac{|obj_{k-1} - obj_k|}{obj_k}$
    \BState \emph{Update projector}
    \State $r_{k} = r_{k-1} - \frac{1}{8 \lambda} \frac{\nabla x_k}{\partial p_{k}}$
    \State $s_{k}= r_{k-1} - \frac{1}{8 \lambda} \frac{\nabla x_k}{\partial q_{k}}$
    \BState \emph{FISTA}
    \State $t_{k+1} = \frac{1 + \sqrt{1 + 4 t_{k}^2}}{2}$
    \State $p_{k} = \frac{r_k}{\text{max}\{1, \sqrt{r_{k}^2 + s_{k}^2 }\}}$
    \State $q_{k} = \frac{s_k}{\text{max}\{1, \sqrt{r_{k}^2 + s_{k}^2 }\}}$
    \If {$tol_k \leq tol$}
        \State \textbf{Stop}
    \EndIf
    \State $k = k +1$
    \EndIf 
\end{algorithmic}
\end{algorithm}

\vspace{0.2in}

\noin Now we discuss other implementation choices in this algorithm. First, we decided to use relative tolerance as our stopping criterion since the stability of the proximal gradient can also be used in a similar way as gradient in a first-order optimization problem. Second, we used L-curve criterion to decide the regularization parameter, because it gives us a principled way to decide the proper amount of regularization without relying on the reference image. 

%\bibliography{ref}{}
%\bibliographystyle{plain}

\end{document}